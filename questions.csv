1; Version space is a subset of your hypothesis space.
2; The target concept is an element of your hypothesis space.
3; One of the powerful advantages of the boosting algorithm is that it does not overfit.
4; Bayesian Networks can represent any propositional statement (i.e., boolean symbols and operators).
5; As the amount of training data increases, the training error goes down
6; A classifier trained on less training data is less likely to overfit.
7; Boosting is used to reduce the bias of low-variance classifiers.
8; We can get multiple local optimum solutions if we solve a linear regression problem by minimizing the sum of squared errors using gradient descent.
9; When the hypothesis space is richer, over ﬁtting is more likely.
10; When the feature space is larger, over ﬁtting is more likely.
11; As the number of training examples goes to infinity, your model trained on that dataset will have the same bias but lower variance.
12; In the class we argued that we can reselect the same variable along one path in a decision tree. Someone argued that this approach is inefficient and unnecessary. This argument is correct whether our attributes are discrete or continuous.
13; The output of a boosting algorithm that learns using decision stumps can be converted to an equivalent decision tree in a straightforward way.
14; Given a network of perceptrons where each perceptron uses linear activation functions (that is, for each unit, j, the output is some constant cj, times the weighted sum of its inputs with no thresholds), one can construct a single unit perceptron that computes the same function.
15; The best learning algorithms make no assumptions.
16; The variance of k-NN decreases as k increases.
17; The ID3 algorithm returns the smallest consistent tree.
18; More training data decreases the training error.
19; Boosting of boosted classifiers leads to even lower bias.
20; Adaboost is a special case of gradient boosting.
21; SVMs involve a convex minimization problem.
